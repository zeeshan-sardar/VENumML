{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encrypted Facial Recognition Demo\n",
    "This Jupyter notebook provides an introduction to performing facial recognition over encrypted images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Vj9-6FyME70I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-02-10 15:04:37 - Directory /Users/zeeshan.sardar/.deepface has been created\n",
      "25-02-10 15:04:37 - Directory /Users/zeeshan.sardar/.deepface/weights has been created\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from venumML.venumpy import small_glwe as vp\n",
    "from scipy.spatial import distance\n",
    "# Dependancies: pip install tensorflow, deepface, scipy\n",
    "\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mlmjxSwXZDTn"
   },
   "outputs": [],
   "source": [
    "# Import the images to perform facial recognition on\n",
    "img1_path = \"hp1.png\"\n",
    "img2_path = \"hp2.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvhSuQl3nUD-"
   },
   "source": [
    "# Image Embeddings\n",
    "\n",
    "Represent images as facenet embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "8WQgFaMyZKbX"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'deepface.modules.modeling' has no attribute 'build_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use the deepface lib to create image embeddings\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m img1_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mDeepFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFacenet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m img2_embedding \u001b[38;5;241m=\u001b[39m DeepFace\u001b[38;5;241m.\u001b[39mrepresent(img2_path, model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFacenet\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/venum_ml/lib/python3.11/site-packages/deepface/DeepFace.py:418\u001b[0m, in \u001b[0;36mrepresent\u001b[0;34m(img_path, model_name, enforce_detection, detector_backend, align, expand_percentage, normalization, anti_spoofing, max_faces)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrepresent\u001b[39m(\n\u001b[1;32m    360\u001b[0m     img_path: Union[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[1;32m    361\u001b[0m     model_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVGG-Face\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m     max_faces: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    369\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    Represent facial images as multi-dimensional vector embeddings.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m            to 'skip', the confidence will be 0 and is nonsensical.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrepresentation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_detection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpand_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43manti_spoofing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manti_spoofing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_faces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_faces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/venum_ml/lib/python3.11/site-packages/deepface/modules/representation.py:68\u001b[0m, in \u001b[0;36mrepresent\u001b[0;34m(img_path, model_name, enforce_detection, detector_backend, align, expand_percentage, normalization, anti_spoofing, max_faces)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mRepresent facial images as multi-dimensional vector embeddings.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m        to 'skip', the confidence will be 0 and is nonsensical.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m resp_objs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 68\u001b[0m model: FacialRecognition \u001b[38;5;241m=\u001b[39m \u001b[43mmodeling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_model\u001b[49m(\n\u001b[1;32m     69\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacial_recognition\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_name\u001b[38;5;241m=\u001b[39mmodel_name\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# ---------------------------------\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# we have run pre-process in verification. so, this can be skipped if it is coming from verify.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m target_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minput_shape\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'deepface.modules.modeling' has no attribute 'build_model'"
     ]
    }
   ],
   "source": [
    "# Use the deepface lib to create image embeddings\n",
    "img1_embedding = DeepFace.represent(img1_path, model_name = 'Facenet')[0][\"embedding\"]\n",
    "img2_embedding = DeepFace.represent(img2_path, model_name = 'Facenet')[0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encryption\n",
    "\n",
    "Apply homomorphic encryption to facial embeddings done on client side.\n",
    "\n",
    "Store the embeddings in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encrypt the image embeddings\n",
    "ctx = vp.SecretContext()\n",
    "ctx.precision= 6\n",
    "c1 = [ctx.encrypt(v) for v in img1_embedding]\n",
    "c2 = [ctx.encrypt(v) for v in img2_embedding]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqiAcPSuoqbg"
   },
   "source": [
    "# Calculations\n",
    "\n",
    "In the cloud, make computations over encrypted facial embeddings.\n",
    "\n",
    "Euclidean Distance Metric: Subtract vectors, then perform dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the encrypted vectors\n",
    "cipher_d = [c2_i - c1_i for c2_i, c1_i in zip(c2, c1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<venumpy.small_glwe.Ciphertext at 0x286bb2430>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a dot product to get the euclidean distance metric\n",
    "euclidean_squared_0 = [cipher_d_i * cipher_d_i for cipher_d_i in cipher_d]\n",
    "\n",
    "total = ctx.encrypt(0.0) #TODO: why?\n",
    "for i in euclidean_squared_0:\n",
    "    total = total + i\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNBqRMR_bT8A"
   },
   "source": [
    "# Decryption\n",
    "\n",
    "\n",
    "Homomorphically encrypted euclidean squared value computed in the cloud, then retrieve it from the client.\n",
    "\n",
    "Only the client can decrypt it because they hold the private key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.186523206818"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decrypt the euclidean distance facial recognition metric\n",
    "decrypted_es = total.decrypt()\n",
    "decrypted_es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrwPeD9g3dp_"
   },
   "source": [
    "# Final Results in plaintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBjfQOGMbdk4",
    "outputId": "1858a1e8-848d-452a-9f4f-307aa8141fe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the same person in different images\n"
     ]
    }
   ],
   "source": [
    "if decrypted_es < 100:\n",
    "    print(\"This is the same person in different images\")\n",
    "else:\n",
    "    print(\"These are different people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hvGCA8RbguZ"
   },
   "source": [
    "# Validation\n",
    "\n",
    "What if euclidean distance calculation is done over plaintext data?\n",
    "\n",
    "The result should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1vX8Qtq2bdnJ",
    "outputId": "3c4e5f46-cfa9-4421-b71e-fa3aceaebd32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euclidean squared - plaintext:  85.1865116552079\n",
      "euclidean squared - homomorphic:  85.186523206818\n",
      "The difference between plaintext and homomorphic is acceptable.\n"
     ]
    }
   ],
   "source": [
    "# Euclidean squared distance\n",
    "euclidean_distance = distance.euclidean(img1_embedding, img2_embedding)\n",
    "plt_distance = euclidean_distance**2\n",
    "\n",
    "print(\"euclidean squared - plaintext: \", plt_distance)\n",
    "print(\"euclidean squared - homomorphic: \", decrypted_es)\n",
    "\n",
    "# Check the difference in precision\n",
    "if abs(plt_distance - decrypted_es) < 0.0001:\n",
    "  print(\"The difference between plaintext and homomorphic is acceptable.\")\n",
    "else:\n",
    "  print(\"The difference between plaintext and homomorphic is unacceptable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venum_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
